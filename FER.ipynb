{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facial Expressions Recognition using Deep Convolutional Neural Networks\n",
    "#### Seminar<br/>Computer Science and Engineering<br/>SJCE<br/>Mysuru\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 7\n",
    "batch_size = 256\n",
    "epochs = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/pradeep/Downloads/all/fer2013/fer2013.csv\") as f:\n",
    "    content = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances :  35888\n",
      "Instance Length :  2304\n"
     ]
    }
   ],
   "source": [
    "lines = np.array(content)\n",
    "num_of_instances = lines.size\n",
    "print(\"Number of instances : \", num_of_instances)\n",
    "print(\"Instance Length : \",len(lines[1].split(\",\")[1].split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train,x_test,y_test = [],[],[],[]\n",
    "for i in range(1,num_of_instances):\n",
    "    try:\n",
    "        emotion,img,usage = lines[i].split(\",\")\n",
    "        val = img.split(\" \")\n",
    "        pixels = np.array(val,'float32')\n",
    "        emotion = keras.utils.to_categorical(emotion,num_classes)\n",
    "        \n",
    "        if 'Training' in usage:\n",
    "            y_train.append(emotion)\n",
    "            x_train.append(pixels)\n",
    "        elif 'PublicTest' in usage:\n",
    "            y_test.append(emotion)\n",
    "            x_test.append(pixels)\n",
    "    except:\n",
    "        print(\"E\",end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train,'float32')\n",
    "y_train = np.array(y_train,'float32')\n",
    "x_test = np.array(x_test,'float32')\n",
    "y_test = np.array(y_test,'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28709, 2304)\n",
      "(28709, 7)\n",
      "(3589, 2304)\n",
      "(3589, 7)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0],48,48,1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.reshape(x_test.shape[0],48,48,1)\n",
    "x_test = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction of CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1st convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(32,(5,5),activation = 'relu',input_shape=(48,48,1)))\n",
    "model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2nd convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(32,(4,4),activation='relu'))\n",
    "model.add(AveragePooling2D(pool_size=(3,3),strides=(2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3rd convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(64,(5,5),activation='relu'))\n",
    "model.add(AveragePooling2D(pool_size=(3,3),strides=(2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fully connected NN\n",
    "model.add(Dense(3072,activation='relu'))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = ImageDataGenerator()\n",
    "train_generator = gen.flow(x_train,y_train,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 44, 44, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 18, 18, 32)        16416     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 64)          51264     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_2 (Average (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3072)              199680    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 21511     \n",
      "=================================================================\n",
      "Total params: 289,703\n",
      "Trainable params: 289,703\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "256/256 [==============================] - 218s 850ms/step - loss: 1.7929 - acc: 0.2582\n",
      "Epoch 2/16\n",
      "256/256 [==============================] - 192s 749ms/step - loss: 1.6477 - acc: 0.3399\n",
      "Epoch 3/16\n",
      "256/256 [==============================] - 191s 746ms/step - loss: 1.5038 - acc: 0.4193\n",
      "Epoch 4/16\n",
      "256/256 [==============================] - 191s 746ms/step - loss: 1.4125 - acc: 0.4575\n",
      "Epoch 5/16\n",
      "256/256 [==============================] - 192s 750ms/step - loss: 1.3412 - acc: 0.4869\n",
      "Epoch 6/16\n",
      "256/256 [==============================] - 191s 746ms/step - loss: 1.2916 - acc: 0.5078\n",
      "Epoch 7/16\n",
      "256/256 [==============================] - 192s 750ms/step - loss: 1.2431 - acc: 0.5288\n",
      "Epoch 8/16\n",
      "256/256 [==============================] - 191s 746ms/step - loss: 1.2134 - acc: 0.5389\n",
      "Epoch 9/16\n",
      "256/256 [==============================] - 191s 746ms/step - loss: 1.1775 - acc: 0.5569\n",
      "Epoch 10/16\n",
      "256/256 [==============================] - 191s 748ms/step - loss: 1.1499 - acc: 0.5666\n",
      "Epoch 11/16\n",
      "256/256 [==============================] - 193s 752ms/step - loss: 1.1332 - acc: 0.5742\n",
      "Epoch 12/16\n",
      "256/256 [==============================] - 192s 750ms/step - loss: 1.1091 - acc: 0.5816\n",
      "Epoch 13/16\n",
      "256/256 [==============================] - 192s 751ms/step - loss: 1.0905 - acc: 0.5909\n",
      "Epoch 14/16\n",
      "256/256 [==============================] - 203s 794ms/step - loss: 1.0726 - acc: 0.5981\n",
      "Epoch 15/16\n",
      "256/256 [==============================] - 227s 885ms/step - loss: 1.0539 - acc: 0.6036\n",
      "Epoch 16/16\n",
      "256/256 [==============================] - 194s 758ms/step - loss: 1.0330 - acc: 0.6118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdf60548198>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator,steps_per_epoch=batch_size,epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('FERModel2.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
